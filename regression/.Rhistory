summary(BeasLogit1)
BeasLogit3 <- glm(Fall2024 ~ poly(LightBINOM, 2) + HerbaceausBINOM + FinesBinom, data = BeasleyPopSurv, family = "binomial")
anova(BeasLogit1, BeasLogit3, test = "Chisq")
pR2(ChildsLogit1)
pR2(BeasLogit1)
BeasLogit3 <- glm(Fall2024 ~ poly(LightBINOM, 2) + HerbaceausBINOM + FinesBinom, data = BeasleyPopSurv, family = "binomial")
anova(BeasLogit1, BeasLogit3, test = "Chisq")
pR2(ChildsLogit1)
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM + HerbaceausBINOM + FinesBINOM, data = ChildsPopSurv, family = "binomial")
summary(ChildsLogit1)
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(BeasleyPopSurv), size = 0.7 * nrow(BeasleyPopSurv))
train_data <- BeasleyPopSurv[train_indices, ]
test_data <- BeasleyPopSurv[-train_indices, ]
observed <- BeasleyPopSurv$Fall2024
#+ Herbaceaus + HerbaceausBINOM + LightBINOM + Light
#BRAP
model <- glm(`Fall2024` ~ LightBINOM + HerbaceausBINOM + FinesBinom  ,
data = BeasleyPopSurv, family = binomial)
# Stepwise model selection (MASS package)
step.model <- stepAIC(model, direction = "both",
trace = FALSE)
coef(model)
coef(step.model)
# Summarize the final selected model
summary(model)
summary(step.model)
# Full model predictions
full_probabilities <- predict(model, test_data, type = "response")
full_predicted <- ifelse(full_probabilities > 0.5, 1, 0)
# Stepwise model predictions
step_probabilities <- predict(step.model, test_data, type = "response")
step_predicted <- ifelse(step_probabilities > 0.5, 1, 0)
# Model accuracy
# Observed outcomes
observed <- test_data$Fall2024
# Accuracy
full_accuracy <- mean(full_predicted == observed)
step_accuracy <- mean(step_predicted == observed)
cat("Full model accuracy:", full_accuracy, "\n")
cat("Stepwise model accuracy:", step_accuracy, "\n")
# if polynomial is significant, include both
#Test it: tell R to give me predicted values
#Childs
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(ChildsPopSurv), size = 0.7 * nrow(BeasleyPopSurv))
train_data <- ChildsPopSurv[train_indices, ]
test_data <- ChildsPopSurv[-train_indices, ]
observed <- ChildsPopSurv$Fall2024
model <- glm(`Fall2024` ~  HerbaceausBINOM + LightBINOM + FinesBINOM,
data = ChildsPopSurv, family = binomial)
# Stepwise model selection (MASS package)
step.model <- stepAIC(model, direction = "both",
trace = FALSE)
coef(model)
coef(step.model)
# Summarize the final selected model
summary(model)
summary(step.model)
# Full model predictions
full_probabilities <- predict(model, test_data, type = "response")
full_predicted <- ifelse(full_probabilities > 0.5, 1, 0)
# Stepwise model predictions
step_probabilities <- predict(step.model, test_data, type = "response")
step_predicted <- ifelse(step_probabilities > 0.5, 1, 0)
# Model accuracy
# Observed outcomes
observed <- test_data$Fall2024
# Accuracy
full_accuracy <- mean(full_predicted == observed)
step_accuracy <- mean(step_predicted == observed)
cat("Full model accuracy:", full_accuracy, "\n")
cat("Stepwise model accuracy:", step_accuracy, "\n")
ggplot(data = ChildsPopSurv, aes(x = FinesBINOM, y = Fall2024)) +
geom_point() +
stat_smooth(method = "glm", color = "purple", se = FALSE,
method.args = list(family = binomial))
ggplot(data = BeasleyPopSurv, aes(x = FinesBinom, y = Fall2024)) +
geom_point() +
stat_smooth(method = "glm", color = "purple", se = FALSE,
method.args = list(family = binomial))
ggplot(data = BeasleyPopSurv, aes(x = LightBINOM, y = Fall2024)) +
geom_point() +
stat_smooth(method = "glm", color = "purple", se = FALSE,
method.args = list(family = binomial))
pred_probs <- predict(BeasLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == mydata$outcome)
#test for accuracy
pred_probs <- predict(BeasLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
library(caret)
install.packages(caret)
install.packages(caret)
library(caret)
install.packages(caret, dependencies = TRUE)
library(caret)library(caret)TRUE
install.packages(caret, dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library(caret)
pred_probs <- predict(BeasLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
#test for accuracy
pred_probs <- predict(BeasLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
library(pROC)
pred_probs <- predict(BeasLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
# Plot ROC curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 3), ")"))
pred_probs <- predict(BeasLogit3, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
#Childs
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM + HerbaceausBINOM + FinesBINOM, data = ChildsPopSurv, family = "binomial")
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
pred_probs <- predict(BeasLogit3, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.3, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
summary(ChildsLogit1)
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM * FinesBINOM, data = ChildsPopSurv, family = "binomial")
summary(ChildsLogit1)
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM * FinesBINOM, data = ChildsPopSurv, family = "binomial")
summary(ChildsLogit1)
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
#Childs
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM + FinesBINOM, data = ChildsPopSurv, family = "binomial")
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.3, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
ChildsLogit1 <- glm(Fall2024 ~ LightBINOM * FinesBINOM, data = ChildsPopSurv, family = "binomial")
summary(ChildsLogit1)
#test for accuracy
pred_probs <- predict(ChildsLogit1, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.4, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
ChildsPopSurv$LightBINOM_squared <- ChildsPopSurv$LightBINOM^2
ChildsPopSurv$FinesBINOM_squared <- ChildsPopSurv$FinesBINOM^2
ChildsPopSurv$LightBINOM_squared <- ChildsPopSurv$LightBINOM^2
ChildsPopSurv$FinesBINOM_squared <- ChildsPopSurv$FinesBINOM^2
ChildsLogitPoly <- glm(Fall2024 ~ LightBINOM + FinesBINOM + LightBINOM_squared + FinesBINOM_squared + LightBINOM_cubed + FinesBINOM_cubed,
data = ChildsPopSurv, family = "binomial")
ChildsPopSurv$LightBINOM_squared <- ChildsPopSurv$LightBINOM^2
ChildsPopSurv$FinesBINOM_squared <- ChildsPopSurv$FinesBINOM^2
ChildsLogitPoly <- glm(Fall2024 ~ LightBINOM + FinesBINOM + LightBINOM_squared + FinesBINOM_squared,
data = ChildsPopSurv, family = "binomial")
summary(ChildsLogitPoly)
pred_probs <- predict(ChildsLogitPoly3, type = "response")
pred_probs <- predict(ChildsLogitPoly, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
cat("Accuracy:", accuracy, "\n")
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
print(conf_matrix)
# Compute AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
pred_probs <- predict(ChildsLogitPoly, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.3, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
cat("Accuracy:", accuracy, "\n")
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
print(conf_matrix)
# Compute AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
# Predict probabilities
pred_probs <- predict(ChildsLogitPoly, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.6, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == ChildsPopSurv$Fall2024)
cat("Accuracy:", accuracy, "\n")
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(ChildsPopSurv$Fall2024))
print(conf_matrix)
# Compute AUC
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
library(dplR)
library(dplyr)
library(kableExtra)
DendroMerge <- read_csv("DendroMergeforR.csv")
library(dplR)
library(dplyr)
library(kableExtra)
DendroMerge <- read_csv("DendroMergeforR.csv")
setwd("~/GitHub/Thesis/Thesis1")
DendroMerge <- read_csv("DendroMergeforR.csv")
DendroMerge <- read.csv("DendroMergeforR.csv")
DendroMerge <- DendroMerge[,-8]
DendroMerge <- DendroMerge %>%
mutate(ID2 = recode(ID2,
"C" = "Childs",
"L" = "Lower Beasley",
"S" = "Sheep",
"U" = "Upper Beasley",
"W" = "West Clear Creek",
))
DendroMerge <- DendroMerge %>%
rename(`Site ID` = `ID2`)
ggplot(DendroMerge, aes(x = PithHand, y = `Diameter (CM)`)) +
labs( x= "Pith Year", y = "Diameter (cm)", title = "Diameter vs Pith Date") +
geom_point(shape=18, size= 3)+
geom_smooth( method = "lm", se = FALSE, size = 1.5)+
scale_y_continuous(
limits = c(0,100)
) +
scale_x_continuous(
limits = c(1985,2020)
)+ theme_minimal()
library(dplyr)
library(ggplot2)
ggplot(DendroMerge, aes(x = PithHand, y = `Diameter (CM)`)) +
labs( x= "Pith Year", y = "Diameter (cm)", title = "Diameter vs Pith Date") +
geom_point(shape=18, size= 3)+
geom_smooth( method = "lm", se = FALSE, size = 1.5)+
scale_y_continuous(
limits = c(0,100)
) +
scale_x_continuous(
limits = c(1985,2020)
)+ theme_minimal()
View(DendroMerge)
ggplot(DendroMerge, aes(x = PithHand, y = DendroMerge$Diameter..CM.)) +
labs( x= "Pith Year", y = "Diameter (cm)", title = "Diameter vs Pith Date") +
geom_point(shape=18, size= 3)+
geom_smooth( method = "lm", se = FALSE, size = 1.5)+
scale_y_continuous(
limits = c(0,100)
) +
scale_x_continuous(
limits = c(1985,2020)
)+ theme_minimal()
p <-lm(data = DendroMerge, PithHand ~ `Diameter (CM)`)
library(ggplot2)
library(dplyr)
library(aod)
library(glm2)
library(MASS)
library(readxl)
install.packages("pscl", repos = "http://cran.us.r-project.org")
library(pscl)
install.packages("caret", dependencies = TRUE)
library(caret)
library(pROC)
library(pscl)
BeasleyPopSurv <- read.csv("regression.csv")
install.packages("caret", dependencies = TRUE)
BeasleyPopSurv <- read.csv("regression.csv")
setwd("~/GitHub/Thesis/Thesis1/regression")
BeasleyPopSurv <- read.csv("regression.csv")
ChildsPopSurv <- read.csv("ChildsRegresssion.csv")
#Create polynomial terms for possible inclusion into logistic model (for bell-shaped model fit)
#For Beasley
#Polynomial
BeasleyPopSurv$LightBINOM <- BeasleyPopSurv$Light^2
BeasleyPopSurv$HerbaceausBINOM <- BeasleyPopSurv$Herbaceaus^2
BeasleyPopSurv$WaterSurfaceElevationBINOM <- BeasleyPopSurv$WaterSurfaceElevation^2
BeasleyPopSurv$FinesBinom <- BeasleyPopSurv$Fines^2
#For Childs
ChildsPopSurv$LightBINOM <- ChildsPopSurv$Light^2
ChildsPopSurv$HerbaceausBINOM <- ChildsPopSurv$Herbaceaus^2
ChildsPopSurv$FinesBINOM <- ChildsPopSurv$Fines^2
#test for accuracy
pred_probs <- predict(BeasLogit3, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.3, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
roc_curve <- roc(ChildsPopSurv$Fall2024, pred_probs)
# Plot ROC curve
plot(roc_curve, col = "blue", main = paste("ROC Curve (AUC =", round(auc_value, 3), ")"))
library(ggplot2)
library(dplyr)
# Get unique site IDs
site_ids <- unique(DendroMerge$`Site ID`)
# Loop through each site ID and generate a plot
for (site in site_ids) {
# Filter data for the current site
site_data <- DendroMerge %>% filter(`Site ID` == site)
# Create the plot
p <- ggplot(site_data, aes(x = PithHand, y = `Diameter (CM)`, color = `Site ID`)) +
labs(x = "Pith Year", y = "Diameter (cm)", title = paste("Diameter vs Pith Date - Site:", site)) +
geom_point(shape = 18, size = 3) +
geom_smooth(method = "lm", se = FALSE, size = 1.5) +
scale_y_continuous(limits = c(0, 100)) +
scale_x_continuous(limits = c(1985, 2020)) +
theme_minimal()
# Save or print the plot
print(p) # To view in the R console
}
library(ggplot2)
library(dplyr)
# Get unique site IDs
site_ids <- unique(DendroMerge$`Site ID`)
# Loop through each site ID and generate a plot
for (site in site_ids) {
# Filter data for the current site
site_data <- DendroMerge %>% filter(`Site ID` == site)
# Create the plot
p <- ggplot(site_data, aes(x = PithHand, y = DendroMerge$Diameter..CM., color = `Site ID`)) +
labs(x = "Pith Year", y = "Diameter (cm)", title = paste("Diameter vs Pith Date - Site:", site)) +
geom_point(shape = 18, size = 3) +
geom_smooth(method = "lm", se = FALSE, size = 1.5) +
scale_y_continuous(limits = c(0, 100)) +
scale_x_continuous(limits = c(1985, 2020)) +
theme_minimal()
# Save or print the plot
print(p) # To view in the R console
}
cat("AUC:", auc_value, "\n")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
auc_value <- auc(roc_curve)
cat("AUC:", auc_value, "\n")
pred_probs <- predict(BeasLogit3, type = "response")
# Convert probabilities to binary predictions (threshold = 0.5)
pred_class <- ifelse(pred_probs > 0.3, 1, 0)
# Compute accuracy
accuracy <- mean(pred_class == BeasleyPopSurv$Fall2024)
# Generate confusion matrix
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(BeasleyPopSurv$Fall2024))
# Compute ROC and AUC
roc_curve <- roc(BeasleyPopSurv$Fall2024, pred_probs)
auc_value <- auc(roc_curve)
# Compute McFadden's Pseudo R²
pseudo_r2 <- pR2(model)["McFadden"]
# Print results
print(conf_matrix)  # Confusion matrix with precision, recall, etc.
cat("Accuracy:", accuracy, "\n")
cat("AUC:", auc_value, "\n")
cat("McFadden's R²:", pseudo_r2, "\n")
